{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction of restaurant ratings: Main Notebook \n",
    "\n",
    "**by Team Tissot: Laux David, Randin Samuel, Solonin Maksim, Vivarié Romin**. \n",
    "\n",
    "The business problem we chose to study is the prediction of restaurants ratings. Having notice that reviews being often biased, we wanted to adress a data mining and machine learning problem to analyse the impact of the different variables on restaurant ratings. Our aim is to study the potential correlations of features for a restaurant's rating and the comparison of different technics.\n",
    "\n",
    "Our first challenge was to select a Data set with enough features and variables to adress the problem. After evaluating various options we chose a Yelp dataset. \n",
    "Yelp gives access to their dataset (https://www.yelp.com/dataset/challenge) for the USA and Canada. The website offers ten awards of $5'000 to the best solutions proposed by students for the following challenges:  \n",
    "- Photo classification  \n",
    "- Natural language processing  \n",
    "- Sentiment Analysis and graph mining  \n",
    "\n",
    "This set includes information about local businesses in 10 metropolitan areas across 2 countries. Round 13 of the challenge was launched on January 15, 2019 and will run through December 31, 2019. \n",
    "\n",
    "After downloading the data, we had to convert it to a csv file. \n",
    "The cleaning taks are compile in a file dedicated, <a href=\"http://localhost:8888/notebooks/Documents/GitHub/DMML2019_Team_Tissot/code/DataCleaning.ipynb\">Cleaning Notebook</a>.  \n",
    "The pre-processin tasks are compiled in a notebook dedicated, the <a href=\"http://localhost:8888/notebooks/Documents/GitHub/DMML2019_Team_Tissot/code/Data%20Pre-processing.ipynb\">Pre-processing Notebook</a>.   \n",
    "  \n",
    "Here is the link to a little <a href=\"https://www.youtube.com/watch?v=Ly7VEcCTke4&t=2s\">video</a> compiling the notebook.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install imblearn -q\n",
    "!pip install keras -q\n",
    "!pip install tensorflow -q\n",
    "!pip install folium -q\n",
    "!pip install seaborn -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get the data\n",
    "Same as in EDA notbook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import folium\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas.io.json import json_normalize\n",
    "import re\n",
    "import seaborn as sns\n",
    "sns.set_style('white')\n",
    "sns.set_color_codes(\"dark\")\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/data_clean_new.csv\", \n",
    "                          encoding='utf_8', \n",
    "                          dtype = 'unicode',\n",
    "                          parse_dates = True,\n",
    "                          infer_datetime_format = True,\n",
    "                          low_memory=False)\n",
    "df = df.drop(\"Unnamed: 0\", axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>address</th>\n",
       "      <th>business_id</th>\n",
       "      <th>city</th>\n",
       "      <th>is_open</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>name</th>\n",
       "      <th>postal_code</th>\n",
       "      <th>review_count</th>\n",
       "      <th>stars</th>\n",
       "      <th>...</th>\n",
       "      <th>Sushi Bars</th>\n",
       "      <th>Tex-Mex</th>\n",
       "      <th>Thai</th>\n",
       "      <th>Vegan</th>\n",
       "      <th>Vegetarian</th>\n",
       "      <th>Vietnamese</th>\n",
       "      <th>Wine &amp; Spirits</th>\n",
       "      <th>Wine Bars</th>\n",
       "      <th>Anymusic</th>\n",
       "      <th>name_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>30 Eglinton Avenue W</td>\n",
       "      <td>QXAEGFB4oINsVuTFxEYKFQ</td>\n",
       "      <td>Mississauga</td>\n",
       "      <td>1</td>\n",
       "      <td>43.6054989743</td>\n",
       "      <td>-79.652288909</td>\n",
       "      <td>Emerald Chinese Restaurant</td>\n",
       "      <td>L5R 3E7</td>\n",
       "      <td>0.014979029358897545</td>\n",
       "      <td>2.5</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10110 Johnston Rd, Ste 15</td>\n",
       "      <td>gnKjwL_1w79qoiV3IC_xQQ</td>\n",
       "      <td>Charlotte</td>\n",
       "      <td>1</td>\n",
       "      <td>35.092564</td>\n",
       "      <td>-80.859132</td>\n",
       "      <td>Musashi Japanese Restaurant</td>\n",
       "      <td>28210</td>\n",
       "      <td>0.020011983223487118</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2450 E Indian School Rd</td>\n",
       "      <td>1Dfx3zM-rW4n-31KeC8sJg</td>\n",
       "      <td>Phoenix</td>\n",
       "      <td>1</td>\n",
       "      <td>33.4951941</td>\n",
       "      <td>-112.0285876</td>\n",
       "      <td>Taco Bell</td>\n",
       "      <td>85016</td>\n",
       "      <td>0.0017974835230677054</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 78 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     address             business_id         city is_open  \\\n",
       "0       30 Eglinton Avenue W  QXAEGFB4oINsVuTFxEYKFQ  Mississauga       1   \n",
       "1  10110 Johnston Rd, Ste 15  gnKjwL_1w79qoiV3IC_xQQ    Charlotte       1   \n",
       "2    2450 E Indian School Rd  1Dfx3zM-rW4n-31KeC8sJg      Phoenix       1   \n",
       "\n",
       "        latitude      longitude                         name postal_code  \\\n",
       "0  43.6054989743  -79.652288909   Emerald Chinese Restaurant     L5R 3E7   \n",
       "1      35.092564     -80.859132  Musashi Japanese Restaurant       28210   \n",
       "2     33.4951941   -112.0285876                    Taco Bell       85016   \n",
       "\n",
       "            review_count stars  ... Sushi Bars Tex-Mex Thai Vegan Vegetarian  \\\n",
       "0   0.014979029358897545   2.5  ...          0       0    0     0          0   \n",
       "1   0.020011983223487118   4.0  ...          1       0    0     0          0   \n",
       "2  0.0017974835230677054   3.0  ...          0       1    0     0          0   \n",
       "\n",
       "  Vietnamese Wine & Spirits Wine Bars Anymusic name_length  \n",
       "0          0              0         0    False          26  \n",
       "1          0              0         0    False          27  \n",
       "2          0              0         0    False           9  \n",
       "\n",
       "[3 rows x 78 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.5    12833\n",
       "4.0    12816\n",
       "3.0     9421\n",
       "4.5     5881\n",
       "2.5     5080\n",
       "2.0     2771\n",
       "1.5      936\n",
       "5.0      931\n",
       "1.0      217\n",
       "Name: stars, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.stars.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert columns to the boolean datatype when we find the vlaue \"True\" or \"False\" in the column\n",
    "for column in df.columns :\n",
    "    if df[column][0] == 'True' or df[column][0] =='False':\n",
    "        df[column] = df[column]=='True'\n",
    "    #Often we find a True or False in the first line\n",
    "    #This tests if the column is a boolean by using the first row for efficiency    \n",
    "        \n",
    "    #otherwise we test if we find a True or False value in the whole column\n",
    "    elif \"True\" in df[column].values :\n",
    "        df[column] = df[column]=='True'\n",
    "    elif \"False\" in df[column].values :\n",
    "        df[column] = df[column]=='True'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuisine_type = [\"American (New)\",\"American (Traditional)\",\"Arts & Entertainment\",\"Asian Fusion\",\"Bakeries\",\"Barbeque\",\"Bars\",\n",
    "\"Beer\",\"Breakfast & Brunch\",\"Buffets\",\"Burgers\",\"Cafes\",\"Canadian (New)\",\"Caribbean\",\"Caterers\",\"Chicken Wings\",\n",
    "\"Chinese\",\"Cocktail Bars\",\"Coffee & Tea\",\"Comfort Food\",\"Delis\",\"Desserts\",\"Diners\",\"Ethnic Food\",\n",
    "\"Event Planning & Services\",\"Fast Food\",\"Food\",\"Food Delivery Services\",\"French\",\"Gastropubs\",\"Gluten-Free\",\n",
    "\"Greek\",\"Grocery\",\"Halal\",\"Hot Dogs\",\"Ice Cream & Frozen Yogurt\",\"Indian\",\"Italian\",\"Japanese\",\"Juice Bars & Smoothies\",\n",
    "\"Korean\",\"Latin American\",\"Lounges\",\"Mediterranean\",\"Mexican\",\"Middle Eastern\",\"Nightlife\",\"Pizza\",\"Pubs\",\n",
    "\"Salad\",\"Sandwiches\",\"Seafood\",\"Soup\",\"Specialty Food\",\"Sports Bars\",\"Steakhouses\",\"Sushi Bars\",\"Tex-Mex\",\n",
    "\"Thai\",\"Vegan\",\"Vegetarian\",\"Vietnamese\",\"Wine & Spirits\",\"Wine Bars\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The cuisine types have 1 or 0 instead of True/False\n",
    "for column in df[cuisine_type] :\n",
    "    df[column] = df[column]==\"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['address', 'business_id', 'city', 'is_open', 'latitude', 'longitude',\n",
       "       'name', 'postal_code', 'review_count', 'stars', 'state', 'Price',\n",
       "       'American (New)', 'American (Traditional)', 'Arts & Entertainment',\n",
       "       'Asian Fusion', 'Bakeries', 'Barbeque', 'Bars', 'Beer',\n",
       "       'Breakfast & Brunch', 'Buffets', 'Burgers', 'Cafes', 'Canadian (New)',\n",
       "       'Caribbean', 'Caterers', 'Chicken Wings', 'Chinese', 'Cocktail Bars',\n",
       "       'Coffee & Tea', 'Comfort Food', 'Delis', 'Desserts', 'Diners',\n",
       "       'Ethnic Food', 'Event Planning & Services', 'Fast Food', 'Food',\n",
       "       'Food Delivery Services', 'French', 'Gastropubs', 'Gluten-Free',\n",
       "       'Greek', 'Grocery', 'Halal', 'Hot Dogs', 'Ice Cream & Frozen Yogurt',\n",
       "       'Indian', 'Italian', 'Japanese', 'Juice Bars & Smoothies', 'Korean',\n",
       "       'Latin American', 'Lounges', 'Mediterranean', 'Mexican',\n",
       "       'Middle Eastern', 'Nightlife', 'Pizza', 'Pubs', 'Salad', 'Sandwiches',\n",
       "       'Seafood', 'Soup', 'Specialty Food', 'Sports Bars', 'Steakhouses',\n",
       "       'Sushi Bars', 'Tex-Mex', 'Thai', 'Vegan', 'Vegetarian', 'Vietnamese',\n",
       "       'Wine & Spirits', 'Wine Bars', 'Anymusic', 'name_length'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: '0.014979029358897545'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-76e6a62131f2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#df[\"Price\"]= df[\"Price\"].astype(int)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"review_count\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"review_count\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"name_length\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"name_length\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"name\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"name\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda3/lib/python3.7/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mastype\u001b[0;34m(self, dtype, copy, errors, **kwargs)\u001b[0m\n\u001b[1;32m   5689\u001b[0m             \u001b[0;31m# else, only a single dtype is given\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5690\u001b[0m             new_data = self._data.astype(dtype=dtype, copy=copy, errors=errors,\n\u001b[0;32m-> 5691\u001b[0;31m                                          **kwargs)\n\u001b[0m\u001b[1;32m   5692\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_constructor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__finalize__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5693\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda3/lib/python3.7/site-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36mastype\u001b[0;34m(self, dtype, **kwargs)\u001b[0m\n\u001b[1;32m    529\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 531\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'astype'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    532\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    533\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda3/lib/python3.7/site-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, f, axes, filter, do_integrity_check, consolidate, **kwargs)\u001b[0m\n\u001b[1;32m    393\u001b[0m                                             copy=align_copy)\n\u001b[1;32m    394\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 395\u001b[0;31m             \u001b[0mapplied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    396\u001b[0m             \u001b[0mresult_blocks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_extend_blocks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mapplied\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult_blocks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda3/lib/python3.7/site-packages/pandas/core/internals/blocks.py\u001b[0m in \u001b[0;36mastype\u001b[0;34m(self, dtype, copy, errors, values, **kwargs)\u001b[0m\n\u001b[1;32m    532\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'raise'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    533\u001b[0m         return self._astype(dtype, copy=copy, errors=errors, values=values,\n\u001b[0;32m--> 534\u001b[0;31m                             **kwargs)\n\u001b[0m\u001b[1;32m    535\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    536\u001b[0m     def _astype(self, dtype, copy=False, errors='raise', values=None,\n",
      "\u001b[0;32m//anaconda3/lib/python3.7/site-packages/pandas/core/internals/blocks.py\u001b[0m in \u001b[0;36m_astype\u001b[0;34m(self, dtype, copy, errors, values, **kwargs)\u001b[0m\n\u001b[1;32m    631\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m                     \u001b[0;31m# _astype_nansafe works fine with 1-d only\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 633\u001b[0;31m                     \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mastype_nansafe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    634\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    635\u001b[0m                 \u001b[0;31m# TODO(extension)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda3/lib/python3.7/site-packages/pandas/core/dtypes/cast.py\u001b[0m in \u001b[0;36mastype_nansafe\u001b[0;34m(arr, dtype, copy, skipna)\u001b[0m\n\u001b[1;32m    681\u001b[0m         \u001b[0;31m# work around NumPy brokenness, #1987\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    682\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0missubdtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minteger\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 683\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype_intsafe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    684\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    685\u001b[0m         \u001b[0;31m# if we have a datetime/timedelta array of objects\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.astype_intsafe\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: invalid literal for int() with base 10: '0.014979029358897545'"
     ]
    }
   ],
   "source": [
    "df['stars']=df['stars'].astype('float')\n",
    "df.Price = pd.to_numeric(df.Price, errors='coerce')\n",
    "df = df[np.isfinite(df['Price'])]\n",
    "#df[\"Price\"]= df[\"Price\"].astype(int)\n",
    "\n",
    "df[\"review_count\"]= df[\"review_count\"].astype(int)\n",
    "df[\"name_length\"]= df[\"name_length\"].astype(int)\n",
    "df[\"name\"]= df[\"name\"].astype(str)\n",
    "df[\"address\"]= df[\"address\"].astype(str)\n",
    "\n",
    "df['latitude'] = df['latitude'].astype(float)\n",
    "df['longitude'] = df['longitude'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Heat map\n",
    "Let's start by visualizing where the restaurants in our dataset are located"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from folium import plugins\n",
    "from folium.plugins import HeatMap\n",
    "\n",
    "\n",
    "# Make an empty map\n",
    "m = folium.Map(location=[28,-90], tiles=\"OpenStreetMap\", zoom_start=4)\n",
    "\n",
    "# Filter the DF for rows, then columns, then remove NaNs\n",
    "heat_df = df[['latitude', 'longitude']]\n",
    "heat_df = heat_df.dropna(axis=0, subset=['latitude','longitude'])\n",
    "\n",
    "# List comprehension to make out list of lists\n",
    "heat_data = [[row['latitude'],row['longitude']] for index, row in heat_df.iterrows()]\n",
    "\n",
    "# Plot it on the map\n",
    "HeatMap(heat_data).add_to(m)\n",
    "\n",
    "# show the map\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that our data is only restaurants from North America."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stars on yelp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will be the value that we want to predict, let's take a look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.stars.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "box_plot_data = df['stars'].astype(\"float\")\n",
    "plt.boxplot(box_plot_data)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that we have an outlier, the 1 star ratings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r1 = df[[\"business_id\",\"stars\"]].groupby([\"stars\"]).count()\n",
    "r1.plot.bar(x=None, \n",
    "            y=None,\n",
    "            figsize=(15,5), \n",
    "            alpha = 0.8, # make the plot 20% transparent\n",
    "            legend = None, \n",
    "           )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Price\n",
    "We think that this will be an important feature so let's get som insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = df.hist(column=\"stars\", by='Price',bins=9, grid=False, figsize=(10,12), layout=(2,2), sharex=True, zorder=2, rwidth=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On this graph we ploted the restaurants price classes (from 1 to 4) by stars.\n",
    "We can see the different distributions of ratings depending on the price category."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cuisine types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2 = df[cuisine_type].sum()\n",
    "r2.plot.bar(x=None, y=None, figsize = (15, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we have a lot of cuisine type we will analyze the top 20\n",
    "We also remove the first 3 cuisine types (Food, Bars and Nightlife) since we don't"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top20_cuisines = list(df[cuisine_type].sum().sort_values(ascending=False).index[0:19])\n",
    "top20_cuisines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df.copy() #create new DataFrame\n",
    "st = df2.loc[:, top20_cuisines].stack()\n",
    "\n",
    "all_ids = pd.Series(st.index.get_level_values(1), \n",
    "                          st.index.get_level_values(0),\n",
    "                          name='top cuisines')[st.values]\n",
    "\n",
    "df2 = df2.join(all_ids, how='left').dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10, 10))\n",
    "sns.boxplot(data = df2[[\"top cuisines\", \"stars\"]], x= \"stars\", y=\"top cuisines\", color=matplotlib.colors.to_hex('#7479e8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that Fast Food, Chicken Wings and Burgers have a lower star ratings median.\n",
    "Cafes have have an above average median."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.stars = df2.stars.astype(float)\n",
    "avg_ratings_cuisine = pd.DataFrame(df2.groupby(\"top cuisines\")[\"stars\"].mean())\n",
    "avg_ratings_cuisine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(data= avg_ratings_cuisine, x = \"stars\", y= avg_ratings_cuisine.index )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base rate\n",
    "The base rate is the size of the most common class divided by the size of the dataset.\n",
    "Our accuracy should be better than the default rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.stars.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The most common class for the ratings is\", df[\"stars\"].mode()[0])\n",
    "\n",
    "baseRate = df[df[\"stars\"] == 3.5].count()[\"stars\"] / df[\"stars\"].count()\n",
    "print(\"The baserate is :\", baseRate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess data for models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The outlier we identified earlier, the 1 star rating, does not contain enough restaurants (only 216), thus we decided to drop it. We also drop string columns, which will not help in prediction (They are mainly related to geography)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "from sklearn import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[df['stars'] != 1.0]\n",
    "X = X.drop(['stars', \"name\", \"address\", \"business_id\", \"city\", \"state\", 'postal_code', 'latitude', 'longitude'], axis = 1)\n",
    "y = df[df[\"stars\"] != 1.0][\"stars\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SMOTE does not handle categorical data, we could also use SMOTE-NC\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.Anymusic = X.Anymusic.astype(bool)\n",
    "X.is_open = X.is_open.astype(bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lab_enc = preprocessing.LabelEncoder()\n",
    "encoded_y = lab_enc.fit_transform(y) #we label encode the star ratings\n",
    "#X.Price = lab_enc.fit_transform(X.Price)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We divide our data into train and test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from pprint import pprint\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, encoded_y, test_size=0.2, random_state=72)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we decided to try two methods to fight severe class imbalance. Just downsampling is not an option (then each class will have 900 observations), thus, first we tried upsampling our train data and the we wanted to try to combine the two methods by upsampling all the classes below the mean and downsampling the classes above it. \n",
    "\n",
    "For upsampling we used a technique called SMOTE (**S**ynthetic **M**inority **O**ver-sampling **TE**chnique) that will synthesize new minority instances. So in our case we will basically generate \"fake\" restaurants based on other on the data set we have, for more details check out : http://rikunert.com/SMOTE_explained .\n",
    "\n",
    "For downsampling we used the NearMiss methode\n",
    "\n",
    "\"first, the method calculates the distances between all instances of the majority class and the instances of the minority class. Then k instances of the majority class that have the smallest distances to those in the minority class are selected. If there are n instances in the minority class, the “nearest” method will result in k*n instances of the majority class.\"\n",
    "\n",
    "source : https://towardsdatascience.com/sampling-techniques-for-extremely-imbalanced-data-part-i-under-sampling-a8dbc3d8d6d8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import NearMiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique, counts = np.unique(encoded_y, return_counts=True)\n",
    "dict(zip(unique, counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#upsamples all the train data\n",
    "X_resampled, y_resampled = SMOTE(random_state = 72).fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "med_cl_cnt= int(pd.Series(y_train).value_counts().median()) # median of obs over classes\n",
    "med_cl_cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique, counts = np.unique(y_train, return_counts=True)\n",
    "dict(zip(unique, counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#downsamples and upsamples the train data (based on median)\n",
    "X_resampled2, y_resampled2 = NearMiss(sampling_strategy = {3:med_cl_cnt, 4:med_cl_cnt, 5:med_cl_cnt, 6:med_cl_cnt}).fit_resample(X_train, y_train) #downsampling\n",
    "X_resampled2, y_resampled2 = SMOTE(random_state = 72, sampling_strategy = {7:med_cl_cnt, 0:med_cl_cnt, 1:med_cl_cnt, 2:med_cl_cnt}).fit_resample(X_resampled2, y_resampled2) # upsampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique, counts = np.unique(y_resampled2, return_counts=True)\n",
    "dict(zip(unique, counts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify parameters values for grid search: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parametersRF = {\n",
    "    'n_estimators': (100,200,300),\n",
    "    'max_depth': (10,20,30)\n",
    "}\n",
    "parametersLR = {\n",
    "    'C': (0.1, 1,100),\n",
    "    'solver': (['saga','lbfgs'])\n",
    "}\n",
    "parametersNN = {\n",
    "    'epochs': ([10, 100]),\n",
    "    'batch_size': ([20,30])\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegressionCV, LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We tried three different samples of data: normal, upsampled, upsampled+downsampled \n",
    "\n",
    "We found that for logistic regression the best result is produced with the normal data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decomment, if need to run best model with other params \n",
    "LR = LogisticRegression(solver='lbfgs',C=0.1, max_iter=2000, multi_class = \"auto\")\n",
    "LR.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # multiprocessing requires the fork to happen in a __main__ protected\n",
    "    # block\n",
    "\n",
    "    # find the best parameters for both the feature extraction and the\n",
    "    # classifier\n",
    "    grid_search = GridSearchCV(LR, parametersLR, cv=2,\n",
    "                               n_jobs=-1, verbose=1,scoring='accuracy')\n",
    "\n",
    "    print(\"Performing grid search...\")\n",
    "    #print(\"pipeline:\", [name for name, _ in pipeline2.steps])\n",
    "    print(\"parameters:\")\n",
    "    pprint(parametersLR)\n",
    "    t0 = time()\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    print(\"done in %0.3fs\" % (time() - t0))\n",
    "    print()\n",
    "\n",
    "    print(\"Best score for Logistic Regression: %0.3f\" % grid_search.best_score_)\n",
    "    print(\"Best parameters set:\")\n",
    "    best_parameters = grid_search.best_estimator_.get_params()\n",
    "    for param_name in sorted(parametersLR.keys()):\n",
    "        print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best model according to gridsearch is the one we tested at the beginning with lbfgs solver and C equal to 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train accuracy\n",
    "LR.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test accuracy\n",
    "LR.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "target_names = [\"1.5\",\"2\",\"2.5\",\"3\",\"3.5\",\"4\",\"4.5\",\"5\"]\n",
    "\n",
    "print(classification_report(y_test, LR.predict(X_test), target_names= target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our test accuracy is above the baserate but it isn't really a good result since we cannot predict 1.5 and 5 stars."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will try both data samples, but still we believe that sticking to one method of upsampling is better (at least more common)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import make_scorer\n",
    "MAE = make_scorer(mean_absolute_error)\n",
    "folds = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(n_estimators = 200,max_depth = 30)\n",
    "clf.fit(X_resampled, y_resampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\": # just for multiprocessing purposes\n",
    "    grid_search = GridSearchCV(clf, parametersRF, cv=3,\n",
    "                               n_jobs=-1, verbose=1,scoring='accuracy')\n",
    "\n",
    "    print(\"Performing grid search...\")\n",
    "    #print(\"Random Forest:\", [name for name, _ in clf.steps])\n",
    "    print(\"parameters:\")\n",
    "    pprint(parametersRF)\n",
    "    t0 = time()\n",
    "    grid_search.fit(X_resampled2, y_resampled2)\n",
    "    print(\"done in %0.3fs\" % (time() - t0))\n",
    "    print()\n",
    "\n",
    "    print(\"Best score for Random Forest: %0.3f\" % grid_search.best_score_)\n",
    "    print(\"Best parameters set:\")\n",
    "    best_parameters = grid_search.best_estimator_.get_params()\n",
    "    for param_name in sorted(parametersRF.keys()):\n",
    "        print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(n_estimators = 300,max_depth = 30)\n",
    "clf.fit(X_resampled, y_resampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf2 = RandomForestClassifier(n_estimators = 300,max_depth = 30)\n",
    "clf2.fit(X_resampled2, y_resampled2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Train accuracy :\", clf.score(X_resampled, y_resampled))\n",
    "print(\"Test accuracy :\", clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Train acc upsample + downsample :\", clf2.score(X_resampled2, y_resampled2))\n",
    "print(\"Test acc upsample + downsample :\", clf2.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Report for upsampling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, clf.predict(X_test), target_names= target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Report for upsampling + downsampling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, clf2.predict(X_test), target_names= target_names)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For random forest upsampling data produces the best result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the end, we would like to use mean absolute error as a performance metric for our models, because predicting rating of the restaurant is a classification with ordinal variable. Thus, misclassification of 0.5 star is better than 1.5 stars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAE_RF = cross_val_score(clf,\n",
    "    X_resampled,\n",
    "    y_resampled,\n",
    "    cv=folds,\n",
    "    scoring=MAE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Random forest MAE :\", np.mean(MAE_RF))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ExtraTreesClassifier(n_estimators=100)\n",
    "model.fit(X_resampled, y_resampled)\n",
    "print(model.feature_importances_) #use inbuilt class feature_importances of tree based classifiers\n",
    "#plot graph of feature importances for better visualization\n",
    "feat_importances = pd.Series(model.feature_importances_, index=X.columns)\n",
    "feat_importances.nlargest(10).plot(kind='barh')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are implementing neural network with basic architecture on not sampled data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras import optimizers\n",
    "from keras.utils import np_utils\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "np.random.seed(1143)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_NN():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(512, input_shape=(69,)))\n",
    "    model.add(Activation('relu')) # An \"activation\" is just a non-linear function applied to the output\n",
    "                              # of the layer above. Here, with a \"rectified linear unit\"\n",
    "                              # we clamp all values below 0 to 0. \n",
    "            \n",
    "    model.add(Dropout(0.2))   # Dropout helps protect the model from memorizing or \"overfitting\" the training data\n",
    "    model.add(Dense(8))\n",
    "    model.add(Activation('softmax')) # This special \"softmax\" activation among other things,\n",
    "                                 # ensures the output is a valid probaility distribution, that is\n",
    "                                 # that its values are all non-negative and sum to 1.\n",
    "    #optimizer = optimizers.Adam(lr=0.01, decay=1e-6)\n",
    "    optimizer = optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KerasClassifier(build_fn= model_NN, epochs=100, batch_size=10, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[df[\"stars\"] != 1]\n",
    "X = X.drop(['stars', \"name\", \"address\", \"business_id\", \"city\", \"state\", 'postal_code', \"longitude\", \"latitude\"], axis = 1)\n",
    "y = df[df[\"stars\"] !=1][\"stars\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nb_classes = len(y.value_counts())\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=72)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "\n",
    "y_train = to_categorical(y_train, num_classes=8)\n",
    "y_test = to_categorical(y_test, num_classes=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_hist = model.fit(X_train, y_train,\n",
    "                       batch_size=64, epochs=10,\n",
    "                       verbose=1, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model.score(X_test, y_test, verbose=0)\n",
    "score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We conducted good EDA for YELP restaurant data and tested three different models to predict the star rating.\n",
    "\n",
    "The best model in terms of stars prediction turned out to be neural network with test accuracy equal to 0.43.\n",
    "\n",
    "Thus, we suggest our friends use this model to understand, how their current restaurant performance will behave in America and estimate their rating among clients."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
